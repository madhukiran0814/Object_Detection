{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be0addd",
   "metadata": {},
   "source": [
    "# Project Sourse Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOLOV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b99965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "obj=ObjectDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315b904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a0cec91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "bicycle\n",
      "car\n",
      "motorbike\n",
      "aeroplane\n",
      "bus\n",
      "train\n",
      "truck\n",
      "boat\n",
      "traffic light\n",
      "fire hydrant\n",
      "stop sign\n",
      "parking meter\n",
      "bench\n",
      "bird\n",
      "cat\n",
      "dog\n",
      "horse\n",
      "sheep\n",
      "cow\n",
      "elephant\n",
      "bear\n",
      "zebra\n",
      "giraffe\n",
      "backpack\n",
      "umbrella\n",
      "handbag\n",
      "tie\n",
      "suitcase\n",
      "frisbee\n",
      "skis\n",
      "snowboard\n",
      "sports ball\n",
      "kite\n",
      "baseball bat\n",
      "baseball glove\n",
      "skateboard\n",
      "surfboard\n",
      "tennis racket\n",
      "bottle\n",
      "wine glass\n",
      "cup\n",
      "fork\n",
      "knife\n",
      "spoon\n",
      "bowl\n",
      "banana\n",
      "apple\n",
      "sandwich\n",
      "orange\n",
      "broccoli\n",
      "carrot\n",
      "hot dog\n",
      "pizza\n",
      "donut\n",
      "cake\n",
      "chair\n",
      "sofa\n",
      "pottedplant\n",
      "bed\n",
      "diningtable\n",
      "toilet\n",
      "tvmonitor\n",
      "laptop\n",
      "mouse\n",
      "remote\n",
      "keyboard\n",
      "cell phone\n",
      "microwave\n",
      "oven\n",
      "toaster\n",
      "sink\n",
      "refrigerator\n",
      "book\n",
      "clock\n",
      "vase\n",
      "scissors\n",
      "teddy bear\n",
      "hair drier\n",
      "toothbrush\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Input\\classes.txt\",\"r\") as f:\n",
    "    x=f.read()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce015e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package imageai.Detection in imageai:\n",
      "\n",
      "NAME\n",
      "    imageai.Detection\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    Custom (package)\n",
      "    YOLO (package)\n",
      "    keras_retinanet (package)\n",
      "\n",
      "SUBMODULES\n",
      "    retinanet_models\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        ObjectDetection\n",
      "        VideoObjectDetection\n",
      "    \n",
      "    class ObjectDetection(builtins.object)\n",
      "     |  This is the object detection class for images in the ImageAI library. It provides support for RetinaNet\n",
      "     |      , YOLOv3 and TinyYOLOv3 object detection networks . After instantiating this class, you can set it's properties and\n",
      "     |  make object detections using it's pre-defined functions.\n",
      "     |  \n",
      "     |  The following functions are required to be called before object detection can be made\n",
      "     |  * setModelPath()\n",
      "     |  * At least of of the following and it must correspond to the model set in the setModelPath()\n",
      "     |  [setModelTypeAsRetinaNet(), setModelTypeAsYOLOv3(), setModelTypeAsTinyYOLOv3()]\n",
      "     |  * loadModel() [This must be called once only before performing object detection]\n",
      "     |  \n",
      "     |  Once the above functions have been called, you can call the detectObjectsFromImage() function of\n",
      "     |  the object detection instance object at anytime to obtain observable objects in any image.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  CustomObjects(self, person=False, bicycle=False, car=False, motorcycle=False, airplane=False, bus=False, train=False, truck=False, boat=False, traffic_light=False, fire_hydrant=False, stop_sign=False, parking_meter=False, bench=False, bird=False, cat=False, dog=False, horse=False, sheep=False, cow=False, elephant=False, bear=False, zebra=False, giraffe=False, backpack=False, umbrella=False, handbag=False, tie=False, suitcase=False, frisbee=False, skis=False, snowboard=False, sports_ball=False, kite=False, baseball_bat=False, baseball_glove=False, skateboard=False, surfboard=False, tennis_racket=False, bottle=False, wine_glass=False, cup=False, fork=False, knife=False, spoon=False, bowl=False, banana=False, apple=False, sandwich=False, orange=False, broccoli=False, carrot=False, hot_dog=False, pizza=False, donut=False, cake=False, chair=False, couch=False, potted_plant=False, bed=False, dining_table=False, toilet=False, tv=False, laptop=False, mouse=False, remote=False, keyboard=False, cell_phone=False, microwave=False, oven=False, toaster=False, sink=False, refrigerator=False, book=False, clock=False, vase=False, scissors=False, teddy_bear=False, hair_dryer=False, toothbrush=False)\n",
      "     |       The 'CustomObjects()' function allows you to handpick the type of objects you want to detect\n",
      "     |       from an image. The objects are pre-initiated in the function variables and predefined as 'False',\n",
      "     |       which you can easily set to true for any number of objects available.  This function\n",
      "     |       returns a dictionary which must be parsed into the 'detectCustomObjectsFromImage()'. Detecting\n",
      "     |        custom objects only happens when you call the function 'detectCustomObjectsFromImage()'\n",
      "     |      \n",
      "     |      \n",
      "     |      * true_values_of_objects (array); Acceptable values are 'True' and False  for all object values present\n",
      "     |      \n",
      "     |      :param boolean_values:\n",
      "     |      :return: custom_objects_dict\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  detectCustomObjectsFromImage(self, input_image='', output_image_path='', input_type='file', output_type='file', extract_detected_objects=False, minimum_percentage_probability=50, display_percentage_probability=True, display_object_name=True, display_box=True, thread_safe=False, custom_objects=None)\n",
      "     |  \n",
      "     |  detectObjectsFromImage(self, input_image='', output_image_path='', input_type='file', output_type='file', extract_detected_objects=False, minimum_percentage_probability=50, display_percentage_probability=True, display_object_name=True, display_box=True, thread_safe=False, custom_objects=None)\n",
      "     |      'detectObjectsFromImage()' function is used to detect objects observable in the given image path:\n",
      "     |              * input_image , which can be a filepath, image numpy array or image file stream\n",
      "     |              * output_image_path (only if output_type = file) , file path to the output image that will contain the detection boxes and label, if output_type=\"file\"\n",
      "     |              * input_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are \"file\", \"array\" and \"stream\"\n",
      "     |              * output_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are \"file\" and \"array\"\n",
      "     |              * extract_detected_objects (optional) , option to save each object detected individually as an image and return an array of the objects' image path.\n",
      "     |              * minimum_percentage_probability (optional, 50 by default) , option to set the minimum percentage probability for nominating a detected object for output.\n",
      "     |              * display_percentage_probability (optional, True by default), option to show or hide the percentage probability of each object in the saved/returned detected image\n",
      "     |              * display_display_object_name (optional, True by default), option to show or hide the name of each object in the saved/returned detected image\n",
      "     |              * thread_safe (optional, False by default), enforce the loaded detection model works across all threads if set to true, made possible by forcing all Tensorflow inference to run on the default graph.\n",
      "     |      \n",
      "     |      \n",
      "     |      The values returned by this function depends on the parameters parsed. The possible values returnable\n",
      "     |      are stated as below\n",
      "     |      - If extract_detected_objects = False or at its default value and output_type = 'file' or\n",
      "     |          at its default value, you must parse in the 'output_image_path' as a string to the path you want\n",
      "     |          the detected image to be saved. Then the function will return:\n",
      "     |          1. an array of dictionaries, with each dictionary corresponding to the objects\n",
      "     |              detected in the image. Each dictionary contains the following property:\n",
      "     |              * name (string)\n",
      "     |              * percentage_probability (float)\n",
      "     |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      "     |      \n",
      "     |      - If extract_detected_objects = False or at its default value and output_type = 'array' ,\n",
      "     |        Then the function will return:\n",
      "     |      \n",
      "     |          1. a numpy array of the detected image\n",
      "     |          2. an array of dictionaries, with each dictionary corresponding to the objects\n",
      "     |              detected in the image. Each dictionary contains the following property:\n",
      "     |              * name (string)\n",
      "     |              * percentage_probability (float)\n",
      "     |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      "     |      \n",
      "     |      - If extract_detected_objects = True and output_type = 'file' or\n",
      "     |          at its default value, you must parse in the 'output_image_path' as a string to the path you want\n",
      "     |          the detected image to be saved. Then the function will return:\n",
      "     |          1. an array of dictionaries, with each dictionary corresponding to the objects\n",
      "     |              detected in the image. Each dictionary contains the following property:\n",
      "     |              * name (string)\n",
      "     |              * percentage_probability (float)\n",
      "     |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      "     |          2. an array of string paths to the image of each object extracted from the image\n",
      "     |      \n",
      "     |      - If extract_detected_objects = True and output_type = 'array', the the function will return:\n",
      "     |          1. a numpy array of the detected image\n",
      "     |          2. an array of dictionaries, with each dictionary corresponding to the objects\n",
      "     |              detected in the image. Each dictionary contains the following property:\n",
      "     |              * name (string)\n",
      "     |              * percentage_probability (float)\n",
      "     |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      "     |          3. an array of numpy arrays of each object detected in the image\n",
      "     |      \n",
      "     |      \n",
      "     |      :param input_image:\n",
      "     |      :param output_image_path:\n",
      "     |      :param input_type:\n",
      "     |      :param output_type:\n",
      "     |      :param extract_detected_objects:\n",
      "     |      :param minimum_percentage_probability:\n",
      "     |      :param display_percentage_probability:\n",
      "     |      :param display_object_name:\n",
      "     |      :param thread_safe:\n",
      "     |      :return image_frame:\n",
      "     |      :return output_objects_array:\n",
      "     |      :return detected_objects_image_array:\n",
      "     |  \n",
      "     |  loadModel(self, detection_speed='normal')\n",
      "     |      'loadModel()' function is required and is used to load the model structure into the program from the file path defined\n",
      "     |      in the setModelPath() function. This function receives an optional value which is \"detection_speed\".\n",
      "     |      The value is used to reduce the time it takes to detect objects in an image, down to about a 10% of the normal time, with\n",
      "     |       with just slight reduction in the number of objects detected.\n",
      "     |      \n",
      "     |      \n",
      "     |      * prediction_speed (optional); Acceptable values are \"normal\", \"fast\", \"faster\", \"fastest\" and \"flash\"\n",
      "     |      \n",
      "     |      :param detection_speed:\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelPath(self, model_path)\n",
      "     |      'setModelPath()' function is required and is used to set the file path to a RetinaNet\n",
      "     |       object detection model trained on the COCO dataset.\n",
      "     |       :param model_path:\n",
      "     |       :return:\n",
      "     |  \n",
      "     |  setModelTypeAsRetinaNet(self)\n",
      "     |      'setModelTypeAsRetinaNet()' is used to set the model type to the RetinaNet model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelTypeAsTinyYOLOv3(self)\n",
      "     |      'setModelTypeAsTinyYOLOv3()' is used to set the model type to the TinyYOLOv3 model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelTypeAsYOLOv3(self)\n",
      "     |      'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class VideoObjectDetection(builtins.object)\n",
      "     |  This is the object detection class for videos and camera live stream inputs in the ImageAI library. It provides support for RetinaNet,\n",
      "     |   YOLOv3 and TinyYOLOv3 object detection networks. After instantiating this class, you can set it's properties and\n",
      "     |   make object detections using it's pre-defined functions.\n",
      "     |  \n",
      "     |   The following functions are required to be called before object detection can be made\n",
      "     |   * setModelPath()\n",
      "     |   * At least of of the following and it must correspond to the model set in the setModelPath()\n",
      "     |    [setModelTypeAsRetinaNet(), setModelTypeAsYOLOv3(), setModelTinyYOLOv3()]\n",
      "     |   * loadModel() [This must be called once only before performing object detection]\n",
      "     |  \n",
      "     |   Once the above functions have been called, you can call the detectObjectsFromVideo() function\n",
      "     |   or the detectCustomObjectsFromVideo() of  the object detection instance object at anytime to\n",
      "     |   obtain observable objects in any video or camera live stream.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  CustomObjects(self, person=False, bicycle=False, car=False, motorcycle=False, airplane=False, bus=False, train=False, truck=False, boat=False, traffic_light=False, fire_hydrant=False, stop_sign=False, parking_meter=False, bench=False, bird=False, cat=False, dog=False, horse=False, sheep=False, cow=False, elephant=False, bear=False, zebra=False, giraffe=False, backpack=False, umbrella=False, handbag=False, tie=False, suitcase=False, frisbee=False, skis=False, snowboard=False, sports_ball=False, kite=False, baseball_bat=False, baseball_glove=False, skateboard=False, surfboard=False, tennis_racket=False, bottle=False, wine_glass=False, cup=False, fork=False, knife=False, spoon=False, bowl=False, banana=False, apple=False, sandwich=False, orange=False, broccoli=False, carrot=False, hot_dog=False, pizza=False, donut=False, cake=False, chair=False, couch=False, potted_plant=False, bed=False, dining_table=False, toilet=False, tv=False, laptop=False, mouse=False, remote=False, keyboard=False, cell_phone=False, microwave=False, oven=False, toaster=False, sink=False, refrigerator=False, book=False, clock=False, vase=False, scissors=False, teddy_bear=False, hair_dryer=False, toothbrush=False)\n",
      "     |       The 'CustomObjects()' function allows you to handpick the type of objects you want to detect\n",
      "     |       from a video. The objects are pre-initiated in the function variables and predefined as 'False',\n",
      "     |       which you can easily set to true for any number of objects available.  This function\n",
      "     |       returns a dictionary which must be parsed into the 'detectCustomObjectsFromVideo()'. Detecting\n",
      "     |        custom objects only happens when you call the function 'detectCustomObjectsFromVideo()'\n",
      "     |      \n",
      "     |      \n",
      "     |      * true_values_of_objects (array); Acceptable values are 'True' and False  for all object values present\n",
      "     |      \n",
      "     |      :param boolean_values:\n",
      "     |      :return: custom_objects_dict\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  detectCustomObjectsFromVideo(self, input_file_path='', camera_input=None, output_file_path='', frames_per_second=20, frame_detection_interval=1, minimum_percentage_probability=50, log_progress=False, display_percentage_probability=True, display_object_name=True, display_box=True, save_detected_video=True, per_frame_function=None, per_second_function=None, per_minute_function=None, video_complete_function=None, return_detected_frame=False, detection_timeout=None, thread_safe=False, custom_objects=None)\n",
      "     |  \n",
      "     |  detectObjectsFromVideo(self, input_file_path='', camera_input=None, output_file_path='', frames_per_second=20, frame_detection_interval=1, minimum_percentage_probability=50, log_progress=False, display_percentage_probability=True, display_object_name=True, display_box=True, save_detected_video=True, per_frame_function=None, per_second_function=None, per_minute_function=None, video_complete_function=None, return_detected_frame=False, detection_timeout=None, thread_safe=False, custom_objects=None)\n",
      "     |              'detectObjectsFromVideo()' function is used to detect objects observable in the given video path or a camera input:\n",
      "     |      * input_file_path , which is the file path to the input video. It is required only if 'camera_input' is not set\n",
      "     |      * camera_input , allows you to parse in camera input for live video detections\n",
      "     |      * output_file_path , which is the path to the output video. It is required only if 'save_detected_video' is not set to False\n",
      "     |      * frames_per_second , which is the number of frames to be used in the output video\n",
      "     |      * frame_detection_interval (optional, 1 by default)  , which is the intervals of frames that will be detected.\n",
      "     |      * minimum_percentage_probability (optional, 50 by default) , option to set the minimum percentage probability for nominating a detected object for output.\n",
      "     |      * log_progress (optional) , which states if the progress of the frame processed is to be logged to console\n",
      "     |      * display_percentage_probability (optional), can be used to hide or show probability scores on the detected video frames\n",
      "     |      * display_object_name (optional), can be used to show or hide object names on the detected video frames\n",
      "     |      * save_save_detected_video (optional, True by default), can be set to or not to save the detected video\n",
      "     |      * per_frame_function (optional), this parameter allows you to parse in a function you will want to execute after each frame of the video is detected. If this parameter is set to a function, after every video  frame is detected, the function will be executed with the following values parsed into it:\n",
      "     |          -- position number of the frame\n",
      "     |          -- an array of dictinaries, with each dictionary corresponding to each object detected. Each dictionary contains 'name', 'percentage_probability' and 'box_points'\n",
      "     |          -- a dictionary with with keys being the name of each unique objects and value are the number of instances of the object present\n",
      "     |          -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fourth value into the function\n",
      "     |      \n",
      "     |      * per_second_function (optional), this parameter allows you to parse in a function you will want to execute after each second of the video is detected. If this parameter is set to a function, after every second of a video is detected, the function will be executed with the following values parsed into it:\n",
      "     |          -- position number of the second\n",
      "     |          -- an array of dictionaries whose keys are position number of each frame present in the last second , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\n",
      "     |          -- an array of dictionaries, with each dictionary corresponding to each frame in the past second, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\n",
      "     |          -- a dictionary with its keys being the name of each unique object detected throughout the past second, and the key values are the average number of instances of the object found in all the frames contained in the past second\n",
      "     |          -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed\n",
      "     |                                                              as the fifth value into the function\n",
      "     |      \n",
      "     |      * per_minute_function (optional), this parameter allows you to parse in a function you will want to execute after each minute of the video is detected. If this parameter is set to a function, after every minute of a video is detected, the function will be executed with the following values parsed into it:\n",
      "     |          -- position number of the minute\n",
      "     |          -- an array of dictionaries whose keys are position number of each frame present in the last minute , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\n",
      "     |      \n",
      "     |          -- an array of dictionaries, with each dictionary corresponding to each frame in the past minute, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\n",
      "     |      \n",
      "     |          -- a dictionary with its keys being the name of each unique object detected throughout the past minute, and the key values are the average number of instances of the object found in all the frames contained in the past minute\n",
      "     |      \n",
      "     |          -- If return_detected_frame is set to True, the numpy array of the detected frame will be parsed as the fifth value into the function\n",
      "     |      \n",
      "     |      * video_complete_function (optional), this parameter allows you to parse in a function you will want to execute after all of the video frames have been detected. If this parameter is set to a function, after all of frames of a video is detected, the function will be executed with the following values parsed into it:\n",
      "     |          -- an array of dictionaries whose keys are position number of each frame present in the entire video , and the value for each key is the array for each frame that contains the dictionaries for each object detected in the frame\n",
      "     |          -- an array of dictionaries, with each dictionary corresponding to each frame in the entire video, and the keys of each dictionary are the name of the number of unique objects detected in each frame, and the key values are the number of instances of the objects found in the frame\n",
      "     |          -- a dictionary with its keys being the name of each unique object detected throughout the entire video, and the key values are the average number of instances of the object found in all the frames contained in the entire video\n",
      "     |      \n",
      "     |      * return_detected_frame (optionally, False by default), option to obtain the return the last detected video frame into the per_per_frame_function, per_per_second_function or per_per_minute_function\n",
      "     |      \n",
      "     |      * detection_timeout (optionally, None by default), option to state the number of seconds of a video that should be detected after which the detection function stop processing the video\n",
      "     |      * thread_safe (optional, False by default), enforce the loaded detection model works across all threads if set to true, made possible by forcing all Tensorflow inference to run on the default graph.\n",
      "     |      \n",
      "     |      \n",
      "     |              :param input_file_path:\n",
      "     |              :param camera_input\n",
      "     |              :param output_file_path:\n",
      "     |              :param save_detected_video:\n",
      "     |              :param frames_per_second:\n",
      "     |              :param frame_detection_interval:\n",
      "     |              :param minimum_percentage_probability:\n",
      "     |              :param log_progress:\n",
      "     |              :param display_percentage_probability:\n",
      "     |              :param display_object_name:\n",
      "     |              :param per_frame_function:\n",
      "     |              :param per_second_function:\n",
      "     |              :param per_minute_function:\n",
      "     |              :param video_complete_function:\n",
      "     |              :param return_detected_frame:\n",
      "     |              :param detection_timeout:\n",
      "     |              :param thread_safe:\n",
      "     |              :return output_video_filepath:\n",
      "     |              :return counting:\n",
      "     |              :return output_objects_array:\n",
      "     |              :return output_objects_count:\n",
      "     |              :return detected_copy:\n",
      "     |              :return this_second_output_object_array:\n",
      "     |              :return this_second_counting_array:\n",
      "     |              :return this_second_counting:\n",
      "     |              :return this_minute_output_object_array:\n",
      "     |              :return this_minute_counting_array:\n",
      "     |              :return this_minute_counting:\n",
      "     |              :return this_video_output_object_array:\n",
      "     |              :return this_video_counting_array:\n",
      "     |              :return this_video_counting:\n",
      "     |  \n",
      "     |  loadModel(self, detection_speed='normal')\n",
      "     |      'loadModel()' function is required and is used to load the model structure into the program from the file path defined\n",
      "     |      in the setModelPath() function. This function receives an optional value which is \"detection_speed\".\n",
      "     |      The value is used to reduce the time it takes to detect objects in an image, down to about a 10% of the normal time, with\n",
      "     |       with just slight reduction in the number of objects detected.\n",
      "     |      \n",
      "     |      \n",
      "     |      * prediction_speed (optional); Acceptable values are \"normal\", \"fast\", \"faster\", \"fastest\" and \"flash\"\n",
      "     |      \n",
      "     |      :param detection_speed:\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelPath(self, model_path)\n",
      "     |      'setModelPath()' function is required and is used to set the file path to a RetinaNet,\n",
      "     |      YOLOv3 or TinyYOLOv3 object detection model trained on the COCO dataset.\n",
      "     |       :param model_path:\n",
      "     |       :return:\n",
      "     |  \n",
      "     |  setModelTypeAsRetinaNet(self)\n",
      "     |      'setModelTypeAsRetinaNet()' is used to set the model type to the RetinaNet model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelTypeAsTinyYOLOv3(self)\n",
      "     |      'setModelTypeAsTinyYOLOv3()' is used to set the model type to the TinyYOLOv3 model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  setModelTypeAsYOLOv3(self)\n",
      "     |      'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\n",
      "     |      for the video object detection instance instance object .\n",
      "     |      :return:\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    c:\\users\\91996\\appdata\\roaming\\python\\python37\\site-packages\\imageai\\detection\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import imageai.Detection as detect\n",
    "help(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae76f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f618a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ObjectDetection in module imageai.Detection object:\n",
      "\n",
      "class ObjectDetection(builtins.object)\n",
      " |  This is the object detection class for images in the ImageAI library. It provides support for RetinaNet\n",
      " |      , YOLOv3 and TinyYOLOv3 object detection networks . After instantiating this class, you can set it's properties and\n",
      " |  make object detections using it's pre-defined functions.\n",
      " |  \n",
      " |  The following functions are required to be called before object detection can be made\n",
      " |  * setModelPath()\n",
      " |  * At least of of the following and it must correspond to the model set in the setModelPath()\n",
      " |  [setModelTypeAsRetinaNet(), setModelTypeAsYOLOv3(), setModelTypeAsTinyYOLOv3()]\n",
      " |  * loadModel() [This must be called once only before performing object detection]\n",
      " |  \n",
      " |  Once the above functions have been called, you can call the detectObjectsFromImage() function of\n",
      " |  the object detection instance object at anytime to obtain observable objects in any image.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  CustomObjects(self, person=False, bicycle=False, car=False, motorcycle=False, airplane=False, bus=False, train=False, truck=False, boat=False, traffic_light=False, fire_hydrant=False, stop_sign=False, parking_meter=False, bench=False, bird=False, cat=False, dog=False, horse=False, sheep=False, cow=False, elephant=False, bear=False, zebra=False, giraffe=False, backpack=False, umbrella=False, handbag=False, tie=False, suitcase=False, frisbee=False, skis=False, snowboard=False, sports_ball=False, kite=False, baseball_bat=False, baseball_glove=False, skateboard=False, surfboard=False, tennis_racket=False, bottle=False, wine_glass=False, cup=False, fork=False, knife=False, spoon=False, bowl=False, banana=False, apple=False, sandwich=False, orange=False, broccoli=False, carrot=False, hot_dog=False, pizza=False, donut=False, cake=False, chair=False, couch=False, potted_plant=False, bed=False, dining_table=False, toilet=False, tv=False, laptop=False, mouse=False, remote=False, keyboard=False, cell_phone=False, microwave=False, oven=False, toaster=False, sink=False, refrigerator=False, book=False, clock=False, vase=False, scissors=False, teddy_bear=False, hair_dryer=False, toothbrush=False)\n",
      " |       The 'CustomObjects()' function allows you to handpick the type of objects you want to detect\n",
      " |       from an image. The objects are pre-initiated in the function variables and predefined as 'False',\n",
      " |       which you can easily set to true for any number of objects available.  This function\n",
      " |       returns a dictionary which must be parsed into the 'detectCustomObjectsFromImage()'. Detecting\n",
      " |        custom objects only happens when you call the function 'detectCustomObjectsFromImage()'\n",
      " |      \n",
      " |      \n",
      " |      * true_values_of_objects (array); Acceptable values are 'True' and False  for all object values present\n",
      " |      \n",
      " |      :param boolean_values:\n",
      " |      :return: custom_objects_dict\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  detectCustomObjectsFromImage(self, input_image='', output_image_path='', input_type='file', output_type='file', extract_detected_objects=False, minimum_percentage_probability=50, display_percentage_probability=True, display_object_name=True, display_box=True, thread_safe=False, custom_objects=None)\n",
      " |  \n",
      " |  detectObjectsFromImage(self, input_image='', output_image_path='', input_type='file', output_type='file', extract_detected_objects=False, minimum_percentage_probability=50, display_percentage_probability=True, display_object_name=True, display_box=True, thread_safe=False, custom_objects=None)\n",
      " |      'detectObjectsFromImage()' function is used to detect objects observable in the given image path:\n",
      " |              * input_image , which can be a filepath, image numpy array or image file stream\n",
      " |              * output_image_path (only if output_type = file) , file path to the output image that will contain the detection boxes and label, if output_type=\"file\"\n",
      " |              * input_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are \"file\", \"array\" and \"stream\"\n",
      " |              * output_type (optional) , file path/numpy array/image file stream of the image. Acceptable values are \"file\" and \"array\"\n",
      " |              * extract_detected_objects (optional) , option to save each object detected individually as an image and return an array of the objects' image path.\n",
      " |              * minimum_percentage_probability (optional, 50 by default) , option to set the minimum percentage probability for nominating a detected object for output.\n",
      " |              * display_percentage_probability (optional, True by default), option to show or hide the percentage probability of each object in the saved/returned detected image\n",
      " |              * display_display_object_name (optional, True by default), option to show or hide the name of each object in the saved/returned detected image\n",
      " |              * thread_safe (optional, False by default), enforce the loaded detection model works across all threads if set to true, made possible by forcing all Tensorflow inference to run on the default graph.\n",
      " |      \n",
      " |      \n",
      " |      The values returned by this function depends on the parameters parsed. The possible values returnable\n",
      " |      are stated as below\n",
      " |      - If extract_detected_objects = False or at its default value and output_type = 'file' or\n",
      " |          at its default value, you must parse in the 'output_image_path' as a string to the path you want\n",
      " |          the detected image to be saved. Then the function will return:\n",
      " |          1. an array of dictionaries, with each dictionary corresponding to the objects\n",
      " |              detected in the image. Each dictionary contains the following property:\n",
      " |              * name (string)\n",
      " |              * percentage_probability (float)\n",
      " |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      " |      \n",
      " |      - If extract_detected_objects = False or at its default value and output_type = 'array' ,\n",
      " |        Then the function will return:\n",
      " |      \n",
      " |          1. a numpy array of the detected image\n",
      " |          2. an array of dictionaries, with each dictionary corresponding to the objects\n",
      " |              detected in the image. Each dictionary contains the following property:\n",
      " |              * name (string)\n",
      " |              * percentage_probability (float)\n",
      " |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      " |      \n",
      " |      - If extract_detected_objects = True and output_type = 'file' or\n",
      " |          at its default value, you must parse in the 'output_image_path' as a string to the path you want\n",
      " |          the detected image to be saved. Then the function will return:\n",
      " |          1. an array of dictionaries, with each dictionary corresponding to the objects\n",
      " |              detected in the image. Each dictionary contains the following property:\n",
      " |              * name (string)\n",
      " |              * percentage_probability (float)\n",
      " |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      " |          2. an array of string paths to the image of each object extracted from the image\n",
      " |      \n",
      " |      - If extract_detected_objects = True and output_type = 'array', the the function will return:\n",
      " |          1. a numpy array of the detected image\n",
      " |          2. an array of dictionaries, with each dictionary corresponding to the objects\n",
      " |              detected in the image. Each dictionary contains the following property:\n",
      " |              * name (string)\n",
      " |              * percentage_probability (float)\n",
      " |              * box_points (list of x1,y1,x2 and y2 coordinates)\n",
      " |          3. an array of numpy arrays of each object detected in the image\n",
      " |      \n",
      " |      \n",
      " |      :param input_image:\n",
      " |      :param output_image_path:\n",
      " |      :param input_type:\n",
      " |      :param output_type:\n",
      " |      :param extract_detected_objects:\n",
      " |      :param minimum_percentage_probability:\n",
      " |      :param display_percentage_probability:\n",
      " |      :param display_object_name:\n",
      " |      :param thread_safe:\n",
      " |      :return image_frame:\n",
      " |      :return output_objects_array:\n",
      " |      :return detected_objects_image_array:\n",
      " |  \n",
      " |  loadModel(self, detection_speed='normal')\n",
      " |      'loadModel()' function is required and is used to load the model structure into the program from the file path defined\n",
      " |      in the setModelPath() function. This function receives an optional value which is \"detection_speed\".\n",
      " |      The value is used to reduce the time it takes to detect objects in an image, down to about a 10% of the normal time, with\n",
      " |       with just slight reduction in the number of objects detected.\n",
      " |      \n",
      " |      \n",
      " |      * prediction_speed (optional); Acceptable values are \"normal\", \"fast\", \"faster\", \"fastest\" and \"flash\"\n",
      " |      \n",
      " |      :param detection_speed:\n",
      " |      :return:\n",
      " |  \n",
      " |  setModelPath(self, model_path)\n",
      " |      'setModelPath()' function is required and is used to set the file path to a RetinaNet\n",
      " |       object detection model trained on the COCO dataset.\n",
      " |       :param model_path:\n",
      " |       :return:\n",
      " |  \n",
      " |  setModelTypeAsRetinaNet(self)\n",
      " |      'setModelTypeAsRetinaNet()' is used to set the model type to the RetinaNet model\n",
      " |      for the video object detection instance instance object .\n",
      " |      :return:\n",
      " |  \n",
      " |  setModelTypeAsTinyYOLOv3(self)\n",
      " |      'setModelTypeAsTinyYOLOv3()' is used to set the model type to the TinyYOLOv3 model\n",
      " |      for the video object detection instance instance object .\n",
      " |      :return:\n",
      " |  \n",
      " |  setModelTypeAsYOLOv3(self)\n",
      " |      'setModelTypeAsYOLOv3()' is used to set the model type to the YOLOv3 model\n",
      " |      for the video object detection instance instance object .\n",
      " |      :return:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9395f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5b54327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus percentage is  99.2685317993164\n",
      "person percentage is  99.37040209770203\n",
      "person percentage is  98.2662558555603\n",
      "person percentage is  77.13596224784851\n",
      "person percentage is  76.51337385177612\n",
      "person percentage is  93.95558834075928\n",
      "person percentage is  98.8694429397583\n",
      "person percentage is  99.75049495697021\n",
      "person percentage is  99.17739629745483\n",
      "person percentage is  97.51378893852234\n",
      "person percentage is  97.43852615356445\n",
      "person percentage is  98.3298122882843\n",
      "person percentage is  95.90625166893005\n",
      "person percentage is  98.74305129051208\n",
      "person percentage is  96.01835608482361\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "obj=ObjectDetection()\n",
    "\n",
    "obj.setModelTypeAsYOLOv3()\n",
    "obj.setModelPath(\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Models\\yolo.h5\")\n",
    "obj.loadModel()\n",
    "\n",
    "preds=obj.detectObjectsFromImage(input_image=\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Input\\prp.jpg\",output_image_path=\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Output\\pnew.jpg\")\n",
    "\n",
    "for i in preds:\n",
    "    print(i['name']+\" percentage is \", i['percentage_probability'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d203504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badb9206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus  :  99.2685317993164  :  [343, 157, 717, 303]\n",
      "person  :  30.46470284461975  :  [119, 184, 839, 385]\n",
      "person  :  99.37040209770203  :  [192, 146, 301, 432]\n",
      "person  :  98.2662558555603  :  [301, 191, 363, 395]\n",
      "person  :  77.13596224784851  :  [367, 209, 425, 382]\n",
      "person  :  76.51337385177612  :  [605, 207, 661, 399]\n",
      "person  :  93.95558834075928  :  [778, 218, 837, 423]\n",
      "person  :  98.8694429397583  :  [3, 133, 117, 496]\n",
      "person  :  99.75049495697021  :  [97, 163, 216, 480]\n",
      "person  :  99.17739629745483  :  [394, 197, 509, 476]\n",
      "person  :  97.51378893852234  :  [528, 177, 611, 465]\n",
      "person  :  97.43852615356445  :  [628, 207, 709, 442]\n",
      "person  :  98.3298122882843  :  [705, 216, 788, 443]\n",
      "person  :  95.90625166893005  :  [813, 216, 879, 459]\n",
      "person  :  98.74305129051208  :  [852, 223, 927, 486]\n",
      "person  :  96.01835608482361  :  [952, 262, 1020, 405]\n",
      "person  :  37.218499183654785  :  [0, 214, 62, 500]\n",
      "person  :  42.79279112815857  :  [104, 180, 129, 219]\n",
      "handbag  :  42.93963015079498  :  [312, 223, 359, 307]\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "path_model = \"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Models\"\n",
    "path_input = \"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Input\"\n",
    "path_output = \"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Output\"\n",
    "\n",
    "input_image=\"prp.jpg\"\n",
    "\n",
    "x=input_image.split(\".\")\n",
    "\n",
    "detector = ObjectDetection()\n",
    "\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "\n",
    "detector.setModelPath( os.path.join(path_model, \"yolo.h5\"))\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(path_input , input_image), output_image_path=os.path.join(path_output , x[0]+\"new.jpg\"), minimum_percentage_probability=30)\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \", eachObject[\"percentage_probability\"], \" : \", eachObject[\"box_points\"] )\n",
    "print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd473cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced43f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a97082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---------------------------------------------------FROM WEB CAM---------------------------------------------------'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"---------------------------------------------------FROM WEB CAM---------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30084a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cam=cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    \n",
    "    _,img=cam.read()\n",
    "    \n",
    "    cv2.imshow(\"\",img)\n",
    "    \n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f1550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "obj=ObjectDetection()\n",
    "\n",
    "obj.setModelTypeAsYOLOv3()\n",
    "obj.setModelPath(\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Models\\yolo.h5\")\n",
    "obj.loadModel()\n",
    "\n",
    "\n",
    "cam=cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    _,img=cam.read()\n",
    "\n",
    "    \n",
    "    #img,preds=obj.detectObjectsFromImage(input_image=img,input_type=\"array\",output_type=\"array\")\n",
    "    \n",
    "    img,preds=obj.detectObjectsFromImage(input_image=img,input_type=\"array\",output_type=\"array\")\n",
    "    \n",
    "    # Displaying the image \n",
    "    cv2.imshow(\"\", img)\n",
    "    \n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10195249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267bfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imageai.Detection import ObjectDetection\n",
    "\n",
    "obj=ObjectDetection()\n",
    "\n",
    "obj.setModelTypeAsYOLOv3()\n",
    "obj.setModelPath(\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Models\\yolo.h5\")\n",
    "obj.loadModel()\n",
    "\n",
    "\n",
    "cam=cv2.VideoCapture(0)\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, 1300)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, 1500)\n",
    "\n",
    "while 1:\n",
    "    _,img=cam.read()\n",
    "    #img,preds=obj.detectObjectsFromImage(input_image=img,input_type=\"array\",output_type=\"array\")\n",
    "    cv2.imwrite(r'C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Input\\p.jpg', 1, img) \n",
    "    \n",
    "    preds = obj.detectObjectsFromImage(input_image=\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Input\\p.jpg\",output_image_path=\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Output\\custom_img.jpg\")\n",
    "    \n",
    "    path = r\"C:\\\\Users\\91996\\Desktop\\Object_Recognition\\Output\\custom_img.jpg\"\n",
    "    # Reading an image in default mode\n",
    "    image = cv2.imread(path)\n",
    "  \n",
    "    # Window name in which image is displayed\n",
    "    window_name = 'image'\n",
    "  \n",
    "    # Using cv2.imshow() method \n",
    "    # Displaying the image \n",
    "    cv2.imshow(window_name, image)\n",
    "    \n",
    "    if cv2.waitKey(1)==27:\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773f2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1448189",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc4955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3122b992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on VideoCapture in module cv2 object:\n",
      "\n",
      "class VideoCapture(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  get(...)\n",
      " |      get(propId) -> retval\n",
      " |      .   @brief Returns the specified VideoCapture property\n",
      " |      .   \n",
      " |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      " |      .       or one from @ref videoio_flags_others\n",
      " |      .       @return Value for the specified property. Value 0 is returned when querying a property that is\n",
      " |      .       not supported by the backend used by the VideoCapture instance.\n",
      " |      .   \n",
      " |      .       @note Reading / writing properties involves many layers. Some unexpected result might happens\n",
      " |      .       along this chain.\n",
      " |      .       @code{.txt}\n",
      " |      .       VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware\n",
      " |      .       @endcode\n",
      " |      .       The returned value might be different from what really used by the device or it could be encoded\n",
      " |      .       using device dependent rules (eg. steps or percentage). Effective behaviour depends from device\n",
      " |      .       driver and API Backend\n",
      " |  \n",
      " |  getBackendName(...)\n",
      " |      getBackendName() -> retval\n",
      " |      .   @brief Returns used backend API name\n",
      " |      .   \n",
      " |      .        @note Stream should be opened.\n",
      " |  \n",
      " |  getExceptionMode(...)\n",
      " |      getExceptionMode() -> retval\n",
      " |      .\n",
      " |  \n",
      " |  grab(...)\n",
      " |      grab() -> retval\n",
      " |      .   @brief Grabs the next frame from video file or capturing device.\n",
      " |      .   \n",
      " |      .       @return `true` (non-zero) in the case of success.\n",
      " |      .   \n",
      " |      .       The method/function grabs the next frame from video file or camera and returns true (non-zero) in\n",
      " |      .       the case of success.\n",
      " |      .   \n",
      " |      .       The primary use of the function is in multi-camera environments, especially when the cameras do not\n",
      " |      .       have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that\n",
      " |      .       call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way\n",
      " |      .       the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames\n",
      " |      .       from different cameras will be closer in time.\n",
      " |      .   \n",
      " |      .       Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the\n",
      " |      .       correct way of retrieving data from it is to call VideoCapture::grab() first and then call\n",
      " |      .       VideoCapture::retrieve() one or more times with different values of the channel parameter.\n",
      " |      .   \n",
      " |      .       @ref tutorial_kinect_openni\n",
      " |  \n",
      " |  isOpened(...)\n",
      " |      isOpened() -> retval\n",
      " |      .   @brief Returns true if video capturing has been initialized already.\n",
      " |      .   \n",
      " |      .       If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns\n",
      " |      .       true.\n",
      " |  \n",
      " |  open(...)\n",
      " |      open(filename[, apiPreference]) -> retval\n",
      " |      .   @brief  Opens a video file or a capturing device or an IP video stream for video capturing.\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference = CAP_ANY)\n",
      " |      .       @return `true` if the file has been successfully opened\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(filename, apiPreference, params) -> retval\n",
      " |      .   @brief  Opens a camera for video capturing\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n",
      " |      .       See cv::VideoCaptureProperties\n",
      " |      .   \n",
      " |      .       @return `true` if the file has been successfully opened\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(index[, apiPreference]) -> retval\n",
      " |      .   @brief  Opens a camera for video capturing\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       Parameters are same as the constructor VideoCapture(int index, int apiPreference = CAP_ANY)\n",
      " |      .       @return `true` if the camera has been successfully opened.\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |      \n",
      " |      \n",
      " |      \n",
      " |      open(index, apiPreference, params) -> retval\n",
      " |      .   @brief Returns true if video capturing has been initialized already.\n",
      " |      .   \n",
      " |      .       @overload\n",
      " |      .   \n",
      " |      .       The `params` parameter allows to specify extra parameters encoded as pairs `(paramId_1, paramValue_1, paramId_2, paramValue_2, ...)`.\n",
      " |      .       See cv::VideoCaptureProperties\n",
      " |      .   \n",
      " |      .       @return `true` if the camera has been successfully opened.\n",
      " |      .   \n",
      " |      .       The method first calls VideoCapture::release to close the already opened file or camera.\n",
      " |  \n",
      " |  read(...)\n",
      " |      read([, image]) -> retval, image\n",
      " |      .   @brief Grabs, decodes and returns the next video frame.\n",
      " |      .   \n",
      " |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      " |      .       @return `false` if no frames has been grabbed\n",
      " |      .   \n",
      " |      .       The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the\n",
      " |      .       most convenient method for reading video files or capturing data from decode and returns the just\n",
      " |      .       grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more\n",
      " |      .       frames in video file), the method returns false and the function returns empty image (with %cv::Mat, test it with Mat::empty()).\n",
      " |      .   \n",
      " |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      " |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      " |      .       cvCloneImage and then do whatever you want with the copy.\n",
      " |  \n",
      " |  release(...)\n",
      " |      release() -> None\n",
      " |      .   @brief Closes video file or capturing device.\n",
      " |      .   \n",
      " |      .       The method is automatically called by subsequent VideoCapture::open and by VideoCapture\n",
      " |      .       destructor.\n",
      " |      .   \n",
      " |      .       The C function also deallocates memory and clears \\*capture pointer.\n",
      " |  \n",
      " |  retrieve(...)\n",
      " |      retrieve([, image[, flag]]) -> retval, image\n",
      " |      .   @brief Decodes and returns the grabbed video frame.\n",
      " |      .   \n",
      " |      .       @param [out] image the video frame is returned here. If no frames has been grabbed the image will be empty.\n",
      " |      .       @param flag it could be a frame index or a driver specific flag\n",
      " |      .       @return `false` if no frames has been grabbed\n",
      " |      .   \n",
      " |      .       The method decodes and returns the just grabbed frame. If no frames has been grabbed\n",
      " |      .       (camera has been disconnected, or there are no more frames in video file), the method returns false\n",
      " |      .       and the function returns an empty image (with %cv::Mat, test it with Mat::empty()).\n",
      " |      .   \n",
      " |      .       @sa read()\n",
      " |      .   \n",
      " |      .       @note In @ref videoio_c \"C API\", functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video\n",
      " |      .       capturing structure. It is not allowed to modify or release the image! You can copy the frame using\n",
      " |      .       cvCloneImage and then do whatever you want with the copy.\n",
      " |  \n",
      " |  set(...)\n",
      " |      set(propId, value) -> retval\n",
      " |      .   @brief Sets a property in the VideoCapture.\n",
      " |      .   \n",
      " |      .       @param propId Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...)\n",
      " |      .       or one from @ref videoio_flags_others\n",
      " |      .       @param value Value of the property.\n",
      " |      .       @return `true` if the property is supported by backend used by the VideoCapture instance.\n",
      " |      .       @note Even if it returns `true` this doesn't ensure that the property\n",
      " |      .       value has been accepted by the capture device. See note in VideoCapture::get()\n",
      " |  \n",
      " |  setExceptionMode(...)\n",
      " |      setExceptionMode(enable) -> None\n",
      " |      .   Switches exceptions mode\n",
      " |      .        *\n",
      " |      .        * methods raise exceptions if not successful instead of returning an error code\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c0dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
